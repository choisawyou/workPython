{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples directory 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-05035e106dbf>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_09\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_09\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_09\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_09\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_09\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_09\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist= input_data.read_data_sets('./mnist/data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch = 100\n",
    "batch_size=100\n",
    "learning_rate = 0.0002\n",
    "n_hidden = 256\n",
    "n_input= 28*28\n",
    "n_noise = 128\n",
    "\n",
    "X= tf.placeholder(tf.float32, [None,n_input])  #>> 실제데이터 입력\n",
    "Z = tf.placeholder(tf.float32, [None,n_noise]) #> noise 입력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 128x256\n",
    "G_W1 = tf.Variable(tf.random_normal([n_noise, n_hidden],stddev=0.01))\n",
    "\n",
    "G_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "#256x784 \n",
    "G_W2 = tf.Variable(tf.random_normal([n_hidden,n_input], stddev=0.01))\n",
    "\n",
    "G_b2 = tf.Variable(tf.zeros([n_input]))\n",
    "D_W1 = tf.Variable(tf.random_normal([n_input,n_hidden], stddev=0.01))\n",
    "\n",
    "D_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "D_W2 = tf.Variable(tf.random_normal([n_hidden, 1], stddev=0.01))\n",
    "D_b2 = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성기 외 판별기\n",
    "# cost function : 확률적 함수\n",
    "# 지금까지 MSE, Enthropy(이건분류일때) 를 사용했음  \n",
    "# KL-Divergence  : GAN, VAE(분포의 차를 확인하는 함수이다 ) 에서는 확률적 함수를 사용한다 .\n",
    "# 생성기 -> noise 가 input 데이터이다,  cost function 으로 KL-divergence 를 사용한다 .  ( 이것들이 GAN 의 특징이다 ) \n",
    "# ( 생성기입력에 noise 가 들어간다는것과 cost function 으로 Kl_divergence 를 사용한다 ) \n",
    "\n",
    "def generator(noise_z):  # 128x128\n",
    "    hidden = tf.nn.relu(tf.matmul(noise_z, G_W1) + G_b1) # 128x256\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden, G_W2) + G_b2) # 128x784 \n",
    "    return output# 784 이미지가 생성된다 \n",
    "\n",
    "def discriminator(inputs): # 여기서는 실제이미지의 분포를 확인하낟 \n",
    "    hidden = tf.nn.relu(tf.matmul(inputs, D_W1)+D_b1)  # input 은 784 \n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden,D_W2)+D_b2)\n",
    "    return output\n",
    "\n",
    "def get_noise(batch_size,n_noise):\n",
    "    return np.random.normal(size=(batch_size, n_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G = generator(Z)  # 제너레이터에서 노이즈를 가지고 이미지를 생성한다 \n",
    "D_gene = discriminator(G)# >> 노이즈로부터 생성된 이미지를 판별한다 = 분포를 확인한다 \n",
    "D_real = discriminator(X) # 실제이미지 X 의 분포를 확인한다\n",
    "\n",
    "# 아래 로그가 붙는이유는? log 로 확률값을 구하는데 정보량때문이다\n",
    "# 확률이 높아지면 정보량이 작아진다. / 확률이 낮아지면 정보량이 커진다 ( tfidf 의 논리를 고려할 것 ) \n",
    "# 가장 적합한 분포를 찾아내야한다 ( 높은 것이 유리하다 )  \n",
    "loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1-D_gene))\n",
    "loss_G = tf.reduce_mean(tf.log(D_gene))# 최대우도 추정법을 사용해서 적합한 분포가 될수있도록 로그를 취해서 평균을 내고 minimize 를 해주게 된다 \n",
    "D_var_list = [D_W1, D_b1, D_W2, D_b2]     # ( 최대우도 추정법이니까 확률을 높일수록 좋다 ) \n",
    "G_var_list = [G_W1, G_b1, G_W2, G_b2]\n",
    "train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D, var_list= D_var_list) # 최대값을 찾아야하므로 미니마이즈쓰고 마이너스를 써줬다 \n",
    "# 분포에 적합한, 높은것을 찾아내야하므로 \n",
    "train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G,var_list=G_var_list) # list 를 쓰는건 minimize 를 할때 역전파 변수를 지정한것이다 \n",
    "%matplotlib inline\n",
    "sess = tf.Session() \n",
    "sess.run(tf.global_variables_initializer())\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "loss_val_D, loss_val_G = 0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 D loss : -0.645 G loss : -2.044\n",
      "Epoch: 0001 D loss : -0.3066 G loss : -2.46\n",
      "Epoch: 0002 D loss : -0.1659 G loss : -2.974\n",
      "Epoch: 0003 D loss : -0.652 G loss : -1.444\n",
      "Epoch: 0004 D loss : -0.3426 G loss : -1.881\n",
      "Epoch: 0005 D loss : -0.2688 G loss : -2.413\n",
      "Epoch: 0006 D loss : -0.3233 G loss : -2.729\n",
      "Epoch: 0007 D loss : -0.2922 G loss : -2.455\n",
      "Epoch: 0008 D loss : -0.2157 G loss : -2.403\n",
      "Epoch: 0009 D loss : -0.2211 G loss : -2.614\n",
      "Epoch: 0010 D loss : -0.2948 G loss : -2.504\n",
      "Epoch: 0011 D loss : -0.3405 G loss : -2.526\n",
      "Epoch: 0012 D loss : -0.4143 G loss : -2.15\n",
      "Epoch: 0013 D loss : -0.4465 G loss : -2.16\n",
      "Epoch: 0014 D loss : -0.4221 G loss : -2.235\n",
      "Epoch: 0015 D loss : -0.2481 G loss : -2.693\n",
      "Epoch: 0016 D loss : -0.3741 G loss : -2.666\n",
      "Epoch: 0017 D loss : -0.3256 G loss : -2.506\n",
      "Epoch: 0018 D loss : -0.3325 G loss : -2.628\n",
      "Epoch: 0019 D loss : -0.3708 G loss : -2.64\n",
      "Epoch: 0020 D loss : -0.289 G loss : -3.015\n",
      "Epoch: 0021 D loss : -0.3997 G loss : -2.881\n",
      "Epoch: 0022 D loss : -0.3529 G loss : -2.803\n",
      "Epoch: 0023 D loss : -0.3897 G loss : -2.683\n",
      "Epoch: 0024 D loss : -0.4292 G loss : -2.428\n",
      "Epoch: 0025 D loss : -0.4404 G loss : -2.793\n",
      "Epoch: 0026 D loss : -0.303 G loss : -2.9\n",
      "Epoch: 0027 D loss : -0.4377 G loss : -2.4\n",
      "Epoch: 0028 D loss : -0.4055 G loss : -2.821\n",
      "Epoch: 0029 D loss : -0.3987 G loss : -2.793\n",
      "Epoch: 0030 D loss : -0.5102 G loss : -2.821\n",
      "Epoch: 0031 D loss : -0.4944 G loss : -2.482\n",
      "Epoch: 0032 D loss : -0.4203 G loss : -2.447\n",
      "Epoch: 0033 D loss : -0.4605 G loss : -2.584\n",
      "Epoch: 0034 D loss : -0.4754 G loss : -2.487\n",
      "Epoch: 0035 D loss : -0.452 G loss : -2.502\n",
      "Epoch: 0036 D loss : -0.4266 G loss : -2.71\n",
      "Epoch: 0037 D loss : -0.4381 G loss : -2.907\n",
      "Epoch: 0038 D loss : -0.5266 G loss : -2.585\n",
      "Epoch: 0039 D loss : -0.6402 G loss : -2.328\n",
      "Epoch: 0040 D loss : -0.4987 G loss : -2.645\n",
      "Epoch: 0041 D loss : -0.5561 G loss : -2.556\n",
      "Epoch: 0042 D loss : -0.6437 G loss : -2.016\n",
      "Epoch: 0043 D loss : -0.5506 G loss : -2.742\n",
      "Epoch: 0044 D loss : -0.441 G loss : -2.302\n",
      "Epoch: 0045 D loss : -0.6002 G loss : -2.409\n",
      "Epoch: 0046 D loss : -0.6895 G loss : -2.165\n",
      "Epoch: 0047 D loss : -0.4778 G loss : -2.546\n",
      "Epoch: 0048 D loss : -0.5324 G loss : -2.398\n",
      "Epoch: 0049 D loss : -0.5821 G loss : -2.282\n",
      "Epoch: 0050 D loss : -0.579 G loss : -2.176\n",
      "Epoch: 0051 D loss : -0.5861 G loss : -2.393\n",
      "Epoch: 0052 D loss : -0.7661 G loss : -2.001\n",
      "Epoch: 0053 D loss : -0.6289 G loss : -2.186\n",
      "Epoch: 0054 D loss : -0.5883 G loss : -2.489\n",
      "Epoch: 0055 D loss : -0.6843 G loss : -2.234\n",
      "Epoch: 0056 D loss : -0.7054 G loss : -2.049\n",
      "Epoch: 0057 D loss : -0.6352 G loss : -2.174\n",
      "Epoch: 0058 D loss : -0.6564 G loss : -2.299\n",
      "Epoch: 0059 D loss : -0.8105 G loss : -2.13\n",
      "Epoch: 0060 D loss : -0.5941 G loss : -2.031\n",
      "Epoch: 0061 D loss : -0.5943 G loss : -2.372\n",
      "Epoch: 0062 D loss : -0.7289 G loss : -1.852\n",
      "Epoch: 0063 D loss : -0.7258 G loss : -2.136\n",
      "Epoch: 0064 D loss : -0.6727 G loss : -1.965\n",
      "Epoch: 0065 D loss : -0.6855 G loss : -1.977\n",
      "Epoch: 0066 D loss : -0.7933 G loss : -1.832\n",
      "Epoch: 0067 D loss : -0.667 G loss : -2.031\n",
      "Epoch: 0068 D loss : -0.6069 G loss : -2.206\n",
      "Epoch: 0069 D loss : -0.8172 G loss : -2.043\n",
      "Epoch: 0070 D loss : -0.6948 G loss : -2.296\n",
      "Epoch: 0071 D loss : -0.7122 G loss : -1.998\n",
      "Epoch: 0072 D loss : -0.7854 G loss : -2.403\n",
      "Epoch: 0073 D loss : -0.8235 G loss : -1.819\n",
      "Epoch: 0074 D loss : -0.7018 G loss : -1.985\n",
      "Epoch: 0075 D loss : -0.6412 G loss : -2.149\n",
      "Epoch: 0076 D loss : -0.6479 G loss : -2.33\n",
      "Epoch: 0077 D loss : -0.8492 G loss : -1.784\n",
      "Epoch: 0078 D loss : -0.6603 G loss : -1.915\n",
      "Epoch: 0079 D loss : -0.7626 G loss : -1.936\n",
      "Epoch: 0080 D loss : -0.6862 G loss : -1.923\n",
      "Epoch: 0081 D loss : -0.6871 G loss : -1.936\n",
      "Epoch: 0082 D loss : -0.592 G loss : -2.11\n",
      "Epoch: 0083 D loss : -0.6617 G loss : -2.05\n",
      "Epoch: 0084 D loss : -0.7481 G loss : -1.902\n",
      "Epoch: 0085 D loss : -0.6327 G loss : -1.97\n",
      "Epoch: 0086 D loss : -0.7607 G loss : -1.798\n",
      "Epoch: 0087 D loss : -0.7745 G loss : -1.965\n",
      "Epoch: 0088 D loss : -0.8075 G loss : -1.954\n",
      "Epoch: 0089 D loss : -0.6629 G loss : -2.122\n",
      "Epoch: 0090 D loss : -0.7031 G loss : -2.006\n",
      "Epoch: 0091 D loss : -0.7434 G loss : -2.075\n",
      "Epoch: 0092 D loss : -0.6829 G loss : -1.906\n",
      "Epoch: 0093 D loss : -0.6597 G loss : -2.172\n",
      "Epoch: 0094 D loss : -0.7031 G loss : -2.025\n",
      "Epoch: 0095 D loss : -0.762 G loss : -2.237\n",
      "Epoch: 0096 D loss : -0.661 G loss : -2.055\n",
      "Epoch: 0097 D loss : -0.5746 G loss : -2.287\n",
      "Epoch: 0098 D loss : -0.7666 G loss : -2.151\n",
      "Epoch: 0099 D loss : -0.5527 G loss : -2.34\n",
      "최적화 완료\n"
     ]
    }
   ],
   "source": [
    "# 이미지 분포의 특징을 추출하는 망이다!!! \n",
    "for epoch in range(total_epoch):                    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "        # 실제 이미지 분포 \n",
    "        _, loss_val_D = sess.run([train_D, loss_D],\n",
    "                                feed_dict = {X: batch_xs, Z: noise})  # noise 가 들어가서 이미지를 생성한다   \n",
    "        _, loss_val_G = sess.run([train_G, loss_G], feed_dict={Z: noise}) # 배치사이즈 128 이니까 이미지가 128x128 로 생성된다 (( 위에 Z placeholder) \n",
    "    print('Epoch:',\"%04d\" % epoch,\n",
    "         'D loss : {:.4}'.format(loss_val_D),\n",
    "         'G loss : {:.4}'.format(loss_val_G))\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)  # 10x128 이다  >> noise 개수가 128 이다. ( n_noise 가 128, 배치사이즈 128) \n",
    "        samples = sess.run(G, feed_dict={Z:noise}) # Z 에 noise 를 주었다 \n",
    "        fig,ax = plt.subplots(1, sample_size, figsize=(sample_size,1))\n",
    "        for i in range(sample_size):\n",
    "            ax[i].set_axis_off()  # noise 128개가 들어가게되면 784 이미지 사이즈로 생성이 된다. \n",
    "            ax[i].imshow(np.reshape(samples[i], (28,28)))  #\n",
    "        plt.savefig('sampels/{}.png'.format(str(epoch).zfill(3)),\n",
    "                       bbpx_inches ='tight')\n",
    "        plt.close(fig)\n",
    "print('최적화 완료')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN \n",
    "## smaples폴더 에 있는 이미지를 보면 처음에는 뭉개져있지만  점점 뚜렷하게 예측하는 것을 볼 수 있다\n",
    "## 중요한것은 noise 를 집어넣었다는 것이다 ( noise 가 어떤 수인지는 모른다 ) \n",
    "## noise 로 부터 이미지를 생성했다 (가중치를 곱해서 나온 generator 가 noise 로 부터 생성했더니 이미지가 나옴 )  # 괄호안이 맞는진모르겠음\n",
    "## vae 보다 더 다양하게 사용가능\n",
    "## 다양하게 활용이 되고 있다 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

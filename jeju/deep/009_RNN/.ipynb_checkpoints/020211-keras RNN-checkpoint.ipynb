{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN : 순서가 있는 데이터의 특징을 추출 -> 하나의 셀로 구성되어있다. \n",
    "#  셀을 계산하고 그 다음 셀의 계산할때, 시간적 지연이 발생한다.\n",
    "# input 데이터 vector 로 들어온다. \n",
    "\n",
    "# 기울기 소실 문제해결하려고 나온 것 -> LSTM \n",
    "# Seq2Seq ( ex/ 왼쪽은 한국어 오른쪽은 영어를 두는 것 )\n",
    "# GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 망으로 하려는 것 : TimeSeries 데이터를 반영,  텍스트마이닝을 다룬다. \n",
    "\n",
    "## TimeSeries 를 위해서 TimeDataGenerator 가 만들어져있다. \n",
    "## 시계열 데이터는 자기상관성을 따진다. ( 10년데이터가 11년데이터에 영향을 주는지 안주는지 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN : 비선형적으로 시계열 분석을 하는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오늘수업 :  timedataGenerator 를 이용한 시계열 데이터, 그리고 텍스트 마이닝 을 해볼것이다.\n",
    "#             (word2vec : onehot 인코딩해서 word 팩토라이즈함)\n",
    "\n",
    "# 텍스트 마이닝 : i have a pen 일 경우 i 가 0 , have 가 1 이런식으로 단어에 번호를 매기고 onehotencoding 한다.\n",
    "#   >>> vectorization : 신경망에서 가중치를 만나 사이즈를 줄여주고 근처에 가까운 단어를 확인한다. ( 가까운 단어는 비슷한 벡터를 가짐)\n",
    "#   >>> word2vec 는 vector 를 가지고 단어 또는 문장을 찾을 수 있게 한다. ( 벡터 훈련시켜서 같이 사용되는 단어를 표현 ) \n",
    "#   >>>> 결국 숫자화되고 그 다음 벡터화되면서 \n",
    "\n",
    "\n",
    "# attention :   ??? \n",
    "\n",
    "# attention 망과 lstm 망을 합쳐서 만든 것 -> NMT( LSTM 망으로 특징을 뽑고, 단어들의 중요도를 고려해서 또 이중학습을 하기때문에 정확해짐)\n",
    "\n",
    "# transformer 망 : attention 망을 깊게해서 bidirection 하게 인코딩? 해서 재표현하게 되는? ?\n",
    "\n",
    "\n",
    "# 번역 챗봇 등을 하는 것 seq2seq \n",
    "# 여기서 발전한게 NMT , BERT 이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN (Recurrent neural network)\n",
    "#  - 순서가 있는 데이터의 특징을 추출\n",
    "#  - cell들이 여러개가 모여있는 구조. cell + cell +...+cell\n",
    "#  - input data도 vector로 들어온다\n",
    "#  - cell 에 가중치가 있어서 데이터가 가중치와 만나서 나오면 특성이 된다. \n",
    "#  - 그 특성이 그 다음셀에 전달이 된다. (state)\n",
    "#  - 그 특성과 input data를 같이 cell에 들어가기 때문에 시간이 걸린다. \n",
    "#  - 계속 셀에 넘겨지면 기울기 소실 문제가 생긴다. \n",
    "#  - 계속 셀에 넘겨지면서 영향력이 누적이되서 과도한 영향력을 받게 된다.\n",
    "#  - 이런 문제를 해결하기 위해서 LSTM   => 회로를 2개를 만들어서 (state c & state h)\n",
    "#  - 두개의 회로로 전달되어서 망을 연결 시키는 것을 seq2seq\n",
    "\n",
    "# seq2seq\n",
    "#  - 전에 생긴 특성이 들어와야 하기 때문에 처음 입력 데이터는 빈 공간으로 만들기\n",
    "#  - GRU ( 계산이 작다. 빈공간을 만들어야 하기 때문에 비싸? 서 만든 것. LSTM과 동일한 역활)\n",
    "\n",
    "\n",
    "# RNN은\n",
    "# 1. Time Series\n",
    "#    - TimeData Generator\n",
    "#    - 시간 데이터를 sequence로 만든다\n",
    "#    - 자기 상관성이 있는지 확인해야한다\n",
    "#    - window ( 그 전 시간들이 그 다음 시간에 영향이 주는것을 보는것. )  <- 이게 sequence로 묶는 것. \n",
    "#         -  (10개를 보고 그 다음시간과 영향을 보고 , 또 그다음 10개를 묶어서 다시 보고 (매년) )  <-- 이걸 하는 게 TimeData Generator\n",
    "#    - 정상성을 뜬 데이터만 사용가능 (분산, 공산성 등이 일정)\n",
    "#        - ex) AR (auto regression : 자기회귀), MA (moving average: 이동 평균법), ARMA , ARIMA? ( 시계열 분석 - integrate사용해서)\n",
    "#    - 비정상성을 '차분'을 해서 정상성을 만든다. \n",
    "#        - 차분 : 뒤에 데이터에서 앞에 데이터를 뺀다 == (결국)미분한다.\n",
    "\n",
    "# 2. Text mining\n",
    "#    - tokonization (단어 하나하나를 자른다)  \n",
    "#    - Dictionary ( 중복된 단어가 없게 )\n",
    "#    - 단어에 번호를 매긴다 \n",
    "#    - 단어에 인덱싱  <= one-hot encoding \n",
    "#    - vectorization을 해준다. (신경망에서 가중치를 만나 사이즈를 줄여주고 근처에 가까운 단어를 확인한다)\n",
    "#             - 가까운 단어는 비슷한? 벡터를 준다 => word to back? vec?\n",
    "#             - vector를 가지고 단어 또는 문장을 찾을 수 있게 한다.\n",
    "#             - word to back : 벡터를 가까이에 있는 단어를 ( 같이 사용되는 단어들을 ) 같이 훈련시킨다?\n",
    "#    - layer 를 지원한다.  \n",
    "#    - 결국 tesxt도 숫자화 되어야 하고 그게 벡터화 되어져서 특징을 추출(가까이에 있는 단어의 유사도)\n",
    "#    - 이렇게 vector된것을 seq2seq를 하면 더 정확해 진다. \n",
    "#   - attention : 데이터가 들어가면 3가지 가중치에서  query/ key/ value 를 가진다.\n",
    "#        - attention point ( 가중치 + key)\n",
    "#        - attention value ( 가중치 + value)\n",
    "#   - NMT : attention망을 가져서 단어들의 중요도를 뽑는다.\n",
    "\n",
    "\n",
    "# > LSTM과 NMT의 중요도롤 같이 사용해서 단어의 특징을 더 잘 추출한다.\n",
    "# > Transformer : attention망을 여러겹 쌓아서 만든것\n",
    "# > BERT는 트랜스포머르 만들어진것. \n",
    "\n",
    "# seq2seq 로 할수 있는것\n",
    "# 1. translation\n",
    "# 2. chatbot\n",
    "# 3. summerization\n",
    "\n",
    "#  * LSTM & BERT : seq2se2의 발전 버전\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시계열데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "numpy.random.seed(7)\n",
    "dataframe = pandas.read_csv('passengers.csv', usecols=[1], engine='python', skipfooter=3) \n",
    "        # pasenger 은 시계열 데이터이다.    # usecols=[1] : 1번 컬럼만 읽어들인다. \n",
    "\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "train_size= int(len(dataset) * 0.67)\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train),len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .window size 로 독립변수와 종속변수를 분리 \n",
    "\n",
    "def create_dataset(dataset, look_back = 1):\n",
    "    dataX, dataY = [],[]\n",
    "    for i in range(len(dataset)-look_back-1):  \n",
    "        a = dataset[i:(i+look_back),0]  # i 맨처음은 0 이니까 0에서 2까지\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])  # 마지막 데이터를 Y 에 추가해준다. \n",
    "    return numpy.array(dataX), numpy.array(dataY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 2  # window size \n",
    "# 벡터1개로부터 종속변수, 독립변수 \n",
    "# 자기상관성을 이용하면 이전 2개를 보면 다음 한개를 알수 있다. \n",
    "\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "textX, textY = create_dataset(test, look_back)\n",
    "print(trainX.shape)  # 전체 사이즈 줄어듬 \n",
    "\n",
    "# 2x8 차원확대 \n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=look_back, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=200, batch_size=2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어제 하우스 집값 예측을 할때 종속변수 13개가 있고, y값1개가 있었다. ( 이미 y 가 나누어져있었음 ) \n",
    "# 근데 시계열 데이터는 y값이 주로 없다. 몇년도부터 인지 쭉 나오는 것인지 딱히 y 변수가 없다. \n",
    "#>>> 데이터 분할 해서 시계열 데이터 형태에 일치 시킨다. \n",
    "\n",
    "trainScore = model.evaluate(trainX, trainY, verbose=0)\n",
    "print('train score: %.2f MSE (%.2f RMSE)' % (trainScore, math.sqrt(trainScore)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시계열데이터2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://medium.com/@cdabakoglu/time-series-forecasting-arima-lstm-prophet-with-python-e73a750a9887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pmdarima import auto_arima                                  #>> autoarima 가 있다(자동으로 arima 갯수를 파악해줌)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('austr.csv',skipfooter=3)\n",
    "df.head()\n",
    "\n",
    "    \n",
    "df.columns=['Month','beer'] #이름바꾸기\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.Month = pd.to_datetime(df.Month)  # Month 에 있는 놈이 시간 데이터로 시간데이터는 문자열로 인식이 된다. 그래서 변환을 해주어야한다.\n",
    "                                     # to_datetime 을 이용해서 시간으로 변환을 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Month')  # 인덱스를 시간으로 변환한다. \n",
    "df.head()\n",
    "df.index.freq=\"MS\"  # freq 는 월단위의 데이터라는 뜻 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df['beer'].plot(figsize=(16,5), title=\"Monthly Beer Production\")\n",
    "ax.set(xlabel='Dates', ylabel='Total Production');\n",
    "\n",
    "# 맥주의 생산량그래프이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트렌드 즉 경향성을 본다. seasonal 은 계절성을 보고 residual 은 기타=잡음\n",
    "# 계절성은 수평적으로 쭉 나타나고, 트렌트는 우상향 하는 모습을 보임 \n",
    "a = seasonal_decompose(df['beer'], model='add')\n",
    "a.plot();\n",
    "\n",
    "# 결과를 보자.\n",
    "# observed : 관측된 데이터 -> 3개로 분리함 ( trend + seasonal + residual)\n",
    "# trend, seasonal, residual 합치면 observed 가 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(16,7))\n",
    "a.seasonal.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_arima(df['beer'], seasonal = True, m=12, max_p=7, max_d=5, max_q=7, max_P=4,\n",
    "           max_D=4, max_Q=4).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[:len(df)-12]\n",
    "test_data = df[len(df)-12:]\n",
    "arima_model=SARIMAX(train_data[\"beer\"], order = (2,1,1), seasonal_order=(4,0,3,12))\n",
    "# AR, MA, ARMA, 이런 것들은 정상성을 띈 데이터를 시계열 분석하는 모델이고,\n",
    "# arima 는 ar+ma 를 합친것으로 비정상데이터의 시계열 분석을 가능하게 한 모델이다\n",
    "# 그리고 stata 에서 만든 stata arima 모형은 sarimax 로 비정상데이터 분석이 가능하도록 한다. \n",
    "arima_result = arima_model.fit()\n",
    "arima_result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

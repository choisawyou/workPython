{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b2f94750697b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mimg_height\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mimg_width\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimg_height\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_image_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Image' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import load_img, img_to_array, save_img\n",
    "\n",
    "# 변환하려는 이미지 경로\n",
    "target_image_path = '../GAN/data/test.jpg'\n",
    "# 스타일 이미지 경로\n",
    "style_reference_image_path = '../GAN/data/images.jpg'\n",
    "\n",
    "# 생성된 사진의 차원\n",
    "width, height = load_img(target_image_path).size\n",
    "img_height = 400\n",
    "img_width = int(width * img_height / height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a0f42c543daa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstyle_reference_image_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m127.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m127.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels),(_,_) = style_reference_image_path\n",
    "train_images = train_images.reshape(train_images.shape[0],28,28,1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5\n",
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x244ca4a08c8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise = tf.random.normal([1,100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=[28,28,1]))\n",
    "    \n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(128,(5,5),strides=(2,2),padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.00074895]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_ouput), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prifix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer, discrimiator_optimizer=discriminator_optimizer,\n",
    "                                generator = generator, discriminator = discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training = True)\n",
    "        \n",
    "        real_output = discriminator(images, training = True)\n",
    "        fake_output = discriminator(generated_images, training = True)\n",
    "        \n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "    gradients_of_generator = gen_tape,gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    gernerator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "        train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as we go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,epoch + 1,seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,epochs,\n",
    "                           seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

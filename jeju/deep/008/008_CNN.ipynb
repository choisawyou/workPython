{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.set_image_data_format('channels_first') # chaccels_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "img_rows, img_cols=28,28\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0],1,28,28)\n",
    "X_test = X_test.reshape(X_test.shape[0],1,28,28)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],28,28,1)\n",
    "X_test = X_test.reshape(X_test.shape[0],28,28,1)\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train shape : {}'.format(X_train.shape))\n",
    "print('X_train samples : {}'.format(X_train.shape[0]))\n",
    "print('X_test samples : {}'.format(X_test.shape[0]))\n",
    "\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# output : 32, (3,3) : 필터 사이즈\n",
    "# 28x28 => (128,26,26,32)\n",
    "# same(28,28) : 모서리 특징 포함\n",
    "model.add(Conv2D(32,(3,3),activation='relu', input_shape=(28,28,1)))\n",
    "# Conv2D(32, (3, 3), padding='valid', input_shape=(28, 28, 1), activation='relu')\n",
    "\n",
    "# (128,24,24,64)\n",
    "# (128,12,12,64)\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())# 12x12x64 => 1차원\n",
    "\n",
    "# FFNN , FC(fully-connected)\n",
    "model.add(Dense(128,activation='relu'))# 9216x128\n",
    "# # 128x128\n",
    "model.add(Dropout(0.5)) # 계산회로만 생략\n",
    "model.add(Dense(10, activation='softmax'))# 128x10\n",
    "# 128x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "\n",
    "# epoch마다 accuracy를 저장\n",
    "class AccuracyHistory(keras.callbacks.Callback):\n",
    "    # 오버라이딩(재정의)\n",
    "    def on_train_begin(self, logs={}): # 훈련시작시 이벤트\n",
    "        self.acc = []\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('accuracy'))\n",
    "        \n",
    "history = AccuracyHistory()\n",
    "\n",
    "# 트레인, test\n",
    "# train, validation_data\n",
    "hist = model.fit(X_train, Y_train, batch_size=batch_size,epochs=epochs, verbose=1, validation_data=(X_test,Y_test), callbacks=[history])\n",
    "score = model.evaluate(X_test,Y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.on_train_begin()\n",
    "hist.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = X_train[0,:,:,0]\n",
    "plt.imshow(image, cmap=plt.cm.Greys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,13), hist.history['accuracy'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 / 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS(hadoop file system) : 모델 구조 전체가 저장\n",
    "# 가중치, 구조, optimization state등이 저장\n",
    "# jsom으로 저장, 구조 , 가중치를 별도로 저장 => web에서 tensorflow 지원\n",
    "# tensorflow.js import  : web에서도 가능\n",
    "model.save('model_mnist.h5')\n",
    "print('모델이 저장되었습니다.')\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model_mnist.h5')\n",
    "print('모델이 로딩되었습니다.')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이어 정보 확인\n",
    "l1 = model.layers[0]\n",
    "l2 = model.layers[1]\n",
    "l1.name\n",
    "l1.input_shape\n",
    "l1.activation\n",
    "l1.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageDataGenertor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "from keras import backend as K\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28,28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28,28,1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=90)\n",
    "# datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
    "#datagen = ImageDataGenerator(width_shift_range=0.2, height_shift_range=0.2)\n",
    "#datagen = ImageDataGenerator(zca_whitening =True)           #>>>>>>>>>>     이미지 백색화 시키는법 -> noise 제거 (변수간 상관도를 없앤다 ( 즉 이미지의 pca 라고 생각하자 ))\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9,\n",
    "    save_to_dir = 'images', save_prefix='aug', save_format='png'):   # 코드의미 : images 폴더에 저장됐는데 증강이 됐다 -> augment = 증강됐다. \n",
    "    for i in range(0,9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(X_batch[i].reshape(28,28), cmap=pyplot.get_cmap('gray'))\n",
    "    pyplot.show()\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10       # cifar10 은 10개로 구성 (airplane, automobile, bird,cat,deer, dog,frog,horse,ship,truc 분류문제)\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Dropout,Flatten, Activation\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "tf.reset_default_graph()\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "print('X_train shape : {}'.format(X_train.shape))\n",
    "print('X_train shape : {}'.format(X_train.shape[0]))\n",
    "print('X_test shape : {}'.format(X_test.shape[0]))\n",
    "\n",
    "NB_CALSSES = 10\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CALSSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CALSSES)\n",
    "\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[5])\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "plt.imshow(X_train[6])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CHANNELS=3\n",
    "IMG_ROWS=32\n",
    "IMG_COLS = 32\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 40\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# residual망의 영향\n",
    "# 32x32x3 => 32x32x32\n",
    "model.add(Conv2D(32,kernel_size = 3, padding='same', input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, kernel_size=3, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64,kernel_size = 3, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64,3,3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size = BATCH_SIZE, epochs=NB_EPOCH, validation_split = VALIDATION_SPLIT, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator( # 똑같은 데이터  : Random 순서\n",
    "featurewise_center=False,\n",
    "samplewise_center = False,\n",
    "featurewise_std_normalization=False,\n",
    "samplewise_std_normalization=False,\n",
    "zca_whitening=False, # 이미지 PCA 노이즈 제거\n",
    "rotation_range=0,\n",
    "width_shift_range=0.1,\n",
    "height_shift_range=0.1,\n",
    "horizontal_flip = True,\n",
    "vertical_flip=False)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
    "                    samples_per_epoch=X_train.shape[0], nb_epoch=NB_EPOCH, verbose=VERBOSE)\n",
    "print('Test score : {}'.format(score[0]))\n",
    "print('Test accuracy : {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model_json)\n",
    "model.save_weights('cifar10_weights.h2', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open('cifar10_architecture.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights('cifar10_weights.h2')\n",
    "print('Loded model from disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문제 accuracy graph 출력\n",
    "# 이미지 cat, dog를 다운로드한 다음 위의 모델로 예측해 보시요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(history.history['accuracy'])\n",
    "# plt.title('Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accruacy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'],loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "from keras.models import model_from_json\n",
    "\n",
    "img_names = ['cat.jpg','dog.jpg']\n",
    "imgs = [resize(imread(img_name), (32,32)).astype('float32') for img_name in img_names]\n",
    "\n",
    "imgs = np.array(imgs) / 255\n",
    "predictions = model.predict_classes(imgs)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전이(transfer learning) 학습 Application\n",
    "# - 반지도학습(일부데이터의 라벨이 무) : knn, Trenductive SVM\n",
    "# - 가중치 활용 : 그대로 적용\n",
    "# - 가중치 중 일부만 활용\n",
    "# - FFNN 부분만 학습해서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trainded, fine-tuning(FFNN부분만 Domain kno....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array # 배열\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "import numpy as np\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications import resnet50\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "filename = 'banana.jpg'\n",
    "original = load_img(filename, target_size=(224,224))\n",
    "print('PIL image size : {}'.format(original.size))\n",
    "plt.imshow(original)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_image = img_to_array(original)\n",
    "plt.imshow(np.uint8(numpy_image))\n",
    "print('numpy array size',numpy_image.shape)\n",
    "# 차원확대 : 여러장 처리\n",
    "# 1장(3차원 -> 4차원으로 확장)\n",
    "image_batch = np.expand_dims(numpy_image, axis=0)\n",
    "print('image batch size',image_batch.shape)\n",
    "\n",
    "# prepare the image for the resnet50 model\n",
    "processed_image = resnet50.preprocess_input(image_batch.copy())\n",
    "# Dense을 이용해서 모델을 생성\n",
    "# 라벨 => 확률\n",
    "resnet_model = resnet50.ResNet50(weights='imagenet')\n",
    "# softmax 확률 (상위 3개로 예측)\n",
    "predictions = resnet_model.predict(processed_image)\n",
    "label = decode_predictions(predictions, top=3) # labelling\n",
    "print(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras\n",
    "# fine-tuning : FFNN부분만학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "import pathlib\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib.image import imread\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# tf.enable_eager_execution()\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "data_dir = 'C:/Users/ICT01_18/Documents/전달/flowers/flower_photos'\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "label_names= {'daisy':0, 'dandelion':1, 'roses':2, 'sunflowers':3, 'tulips':4}\n",
    "label_key = ['daisy','dandelion','roses','sunflowers','tulips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = list(data_dir.glob(\"*/*\"))\n",
    "all_images = [str(path) for path in all_images]\n",
    "\n",
    "\n",
    "random.shuffle(all_images)\n",
    "all_labels = [label_names[pathlib.Path(path).parent.name] for path in all_images]\n",
    "data_size = len(all_images)\n",
    "train_test_split=(int)(data_size*0.2)\n",
    "x_train=all_images[train_test_split:]\n",
    "x_test=all_images[:train_test_split]\n",
    "y_train=all_labels[train_test_split:]\n",
    "y_test = all_labels[:train_test_split]\n",
    "IMG_SIZE=160\n",
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_data(x,y):\n",
    "    image = tf.read_file(x)\n",
    "    image = tf.image.decode_jpeg(image, channels= 3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image/127.5) -1 \n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image,y \n",
    "\n",
    "\n",
    "def _input_fn(x,y):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    ds = ds.map(_parse_data)\n",
    "    ds = ds.shuffle(buffer_size=data_size)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "train_ds = _input_fn(x_train,y_train)\n",
    "validation_ds = _input_fn(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "VGG16_MODEL = tf.keras.applications.VGG16(input_shape = IMG_SHAPE, include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_MODEL.trainable=False\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(len(label_names), activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([VGG16_MODEL, global_average_layer, prediction_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(), loss = tf.keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2 steps\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument: {{function_node __inference_Dataset_map__parse_data_805}} Invalid JPEG data or crop window, data size 2636\n\t [[{{node DecodeJpeg}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_195]]\n  (1) Invalid argument: {{function_node __inference_Dataset_map__parse_data_805}} Invalid JPEG data or crop window, data size 2636\n\t [[{{node DecodeJpeg}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-30fa9990e22f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    298\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m             \u001b[0mactual_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: {{function_node __inference_Dataset_map__parse_data_805}} Invalid JPEG data or crop window, data size 2636\n\t [[{{node DecodeJpeg}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_195]]\n  (1) Invalid argument: {{function_node __inference_Dataset_map__parse_data_805}} Invalid JPEG data or crop window, data size 2636\n\t [[{{node DecodeJpeg}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=100, steps_per_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

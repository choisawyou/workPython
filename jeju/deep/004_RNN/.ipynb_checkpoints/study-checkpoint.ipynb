{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 3 # 입력데이터\n",
    "n_neurons = 5 # 셀의 가중치 사이즈\n",
    "tf.reset_default_graph() # 그래프 초기화(변수 생성)\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, [None, n_inputs]) # 4x3 만단데이터 개수가 3\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs]) # 4x3\n",
    "# [0,1,2] 사이즈가 하나의 셀로 입력\n",
    "# FFNN(feed foweard neural network)\n",
    "\n",
    "# 가중치 사이즈 => 특성을 찾아내는 것\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "\n",
    "# static_rnn : rnn network : 4개의 셀이 자동으로 연결되면서 메모리 확보\n",
    "output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, [X0,X1], dtype=tf.float32)\n",
    "\n",
    "Y0,Y1 = output_seqs\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "# 출력, 다음셀로 전달되는 값 : 마지막 states값(수평으로 셀을 연결)\n",
    "X0_batch = np.array([[0,1,2],[3,4,5],[6,7,8],[9,0,1]]) # 4x3\n",
    "X1_batch = np.array([[9,8,7],[0,0,0],[6,5,4],[3,2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처음 데이터 특성 : \n",
      "[[ 0.93772817  0.6911686  -0.92254424 -0.9241967  -0.47048622]\n",
      " [ 0.99935037  0.98921055 -0.9999766  -0.99997246 -0.9880695 ]\n",
      " [ 0.9999936   0.9996777  -1.         -1.         -0.99979997]\n",
      " [-0.96237254 -0.88319737 -0.9945716  -0.9758992  -0.95671624]] \n",
      "차수 : (4, 5)\n",
      "\n",
      "두번째 데이터 특성 : \n",
      "[[ 0.9998768   0.9999585  -1.         -1.         -0.9999947 ]\n",
      " [-0.42606276  0.84562737  0.30121    -0.28730503 -0.74988186]\n",
      " [ 0.98416054  0.9992036  -0.99999225 -0.9999967  -0.9997819 ]\n",
      " [ 0.22093709  0.7358358  -0.998526   -0.9997438  -0.7421815 ]] \n",
      "차수 : (4, 5)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0,Y1], feed_dict={X0:X0_batch, X1:X1_batch})\n",
    "    \n",
    "    print('처음 데이터 특성 : \\n{} \\n차수 : {}\\n'.format(Y0_val, Y0_val.shape))\n",
    "    print('두번째 데이터 특성 : \\n{} \\n차수 : {}'.format(Y1_val, Y1_val.shape))\n",
    "    # 4x3 => 4x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? 셀이 가지고있느 가중치 사이즈 : 3x5??\n",
    "왜 5개라고 정해진걸까? => 인풋과 뉴런수에의해 결정된거같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # 그래프 초기화\n",
    "n_steps = 28 # 셀수 \n",
    "n_inputs = 28 # 셀당 인풋사이즈\n",
    "n_neurons = 150 # 누런수\n",
    "n_outputs = 10 # 확률사이즈\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 3차원으로 데이터를 받아오기 때문에\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs]) # ?x28x28\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# FFNN\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units = n_neurons) #150\n",
    "# 데이터가 3차원으로 입력됨\n",
    "# 2차원이 셀수\n",
    "# 셀 : 28개\n",
    "# state 마지막셀의 수평으로 전달되는 값\n",
    "# state마지막셀의 output과 같다.\n",
    "# 28개의 셀이 있는데 마지막 한개의 output을 사용 => many to one\n",
    "# 감정분류 -> 분류\n",
    "# state의 차수 : 150x150\n",
    "# output의 차수 : latent time 지연시간을 통해 계산된셀의\n",
    "# 모든 값을 결합 출력 150x28x150\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "# outputs.shape = > 150x28x150\n",
    "# states.shape => 150x150\n",
    "\n",
    "# 150개의 특징중에 10개만 추출\n",
    "# dense 입력차수, 출력차수만 지정하면 자동으로 바이어스를 생성\n",
    "# 가중치 공간을 확보해서\n",
    "# 150x150, => 150x10\n",
    "# dense의 가중치 사이즈 = 150x10\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "# 분류를 위한 미분이 가능한 식으로 바뀜\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = Y, logits=logits)\n",
    "# 배치 사이즈 => 평균을 통해서 loss를 구한다.\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "# momentum, propgrad\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# 양쪽에서 큰놈으로\n",
    "# 가장큰값\n",
    "correct = tf.nn.in_top_k(logits, Y,1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data/')\n",
    "# 원래이미지 모양으로 학습\n",
    "X_test = mnist.test.images.reshape((-1,n_steps,n_inputs))\n",
    "Y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 Train accuracy : 0.9266666769981384 Test accuracy : 0.9208999872207642\n",
      "epoch 1 Train accuracy : 0.9399999976158142 Test accuracy : 0.9422000050544739\n",
      "epoch 2 Train accuracy : 0.9800000190734863 Test accuracy : 0.9509999752044678\n",
      "epoch 3 Train accuracy : 0.9733333587646484 Test accuracy : 0.9577000141143799\n",
      "epoch 4 Train accuracy : 0.9866666793823242 Test accuracy : 0.963100016117096\n",
      "epoch 5 Train accuracy : 0.9800000190734863 Test accuracy : 0.9656999707221985\n",
      "epoch 6 Train accuracy : 0.9800000190734863 Test accuracy : 0.970300018787384\n",
      "epoch 7 Train accuracy : 0.9866666793823242 Test accuracy : 0.968999981880188\n",
      "epoch 8 Train accuracy : 0.95333331823349 Test accuracy : 0.97079998254776\n",
      "epoch 9 Train accuracy : 0.9866666793823242 Test accuracy : 0.9725000262260437\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        # 60000/150 을 10번 반복해서 돈다.\n",
    "        for iteration in range(mnist.train.num_examples // batch_size): \n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch = X_batch.reshape((-1,n_steps, n_inputs))\n",
    "            sess.run(training_op, feed_dict={X:X_batch,Y:Y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X:X_batch,Y:Y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict = {X:X_test, Y:Y_test})\n",
    "        print('epoch {} Train accuracy : {} Test accuracy : {}'.format(epoch, acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi rnn cell\n",
    "- MultiRNNCell : 3단계로 구성되어있다.(basic rnn cell, multi rnn cell, dynamic_rnn으로 수성)\n",
    "- BasicRNNCell : 기본데이터가 입력되는 곳은 basic rnn cell 이다.!!!\n",
    "- MultiRNNCell : 수직으로 레이어 구성(아까는 이거 없었음) 수평으로 구성되어있었는데 수직으로 구성 >>밑에보면 3개로 -> multi rnn cell로 의해서 수직으로 셀이 구성\n",
    "- dynamic_rnn : 이것이 모델이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "n_steps = 28\n",
    "n_inputs=28\n",
    "n_outputs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "n_neurons = 100\n",
    "n_layers = 3 # 3개의 멀티레이어\n",
    "\n",
    "# basic rnn cell은 3개만들어짐 for문을 통해 확인가능 -> 3개의 셀 생성\n",
    "layers = [tf.contrib.rnn.BasicRNNCell(num_units = n_neurons, activation= tf.nn.relu) for layer in range(n_layers)]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers) # 3개의 셀을 조합해서 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번엔 dynamic_rnn이다. : 셀로 입력되는 데이터의 개수에 맞추어서 셀을 구성할 수 있다.\n",
    "- ex) 나는 학교에 간다.(2글자 + 3글자 + 2글자 -> 사이즈가 다르다. -> 사이즈가 다르면 static_rnn에서 동일한 사이즈로 맞췄다 -> 큰것을 기준으로 작은것을 padding했었다. -> 결과가 안좋게 나오기 때문에 dynamic_rnn을 쓴다.)\n",
    "- dynamic_rnn은 사이즈가 다르면 다른데로 상관하지 않는다.(입력사이즈를 변동 -> 가중치를 조절한다. -> 나가는 특징은 일치함 = neural수는 같다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3층 셀이 28개가 조성이 된다. -> 고정사이즈\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
    "\n",
    "# states가 몇개가 발생하는가?\n",
    "# 3x150x100(층이 3층이므로 3, 배치사이즈가 150, 150x100에 3층이므로 3x150x100)\n",
    "# concat : 열방향으로 하게됨(axis=1에 의해서) -> 150x300이 된다.\n",
    "states_concat = tf.concat(axis = 1, values=states)\n",
    "# dense : 입력차수와 출력차수를 넣어주면 가중치 사이즈를 자기가 만든다.\n",
    "# 150x300이 들어오고, 출력 차수가 10이니까 가중치는 300x10이됨\n",
    "logits=tf.layers.dense(states_concat, n_outputs)\n",
    "\n",
    "# 가충치계산해서 나온결과는 150(미니배치 사이즈)x10(확률적으로 나타나는 10개의 특징 = 확률값))\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y,  logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits,Y,1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
